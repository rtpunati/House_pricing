str(house.df)
####### partition data into training and validation
set.seed(2)   # random sample can be reproduced by setting a value for seed
## sample 60% of data
train.index <- sample(c(1:dim(house.df)[1]), dim(house.df)[1]*0.6)    # dim(house.df)[1] gives the no. of obs.
## create training data
train.df <- house.df[train.index, ]
## create validation data
valid.df <- house.df[-train.index, ]    # "-" works for numbers; otherwise setdiff
str(train.df)
str(valid.df)
##### (1): Full model
##### Full model with one layer and 2 neurons (hidden = 2 is just for simplicity, can try more neurons)
##### Full model with all predictors may take a long time, and may not converge
nn <- neuralnet(price1 + price0 ~ . -price , data = train.df, linear.output = F, hidden = 2)
### plot NN network
plot(nn, rep="best")
#### display NN weights
# names(nn)
nn$weights
### classification matrix: training data
pred <- predict(nn, train.df)
#### "pred" has 2 columns: prob in class 1; and prob in class 0
head(pred)
summary(pred)
### generate the classification matrix
c.mat <- table(ifelse(pred[,1] > 0.5, 1, 0), train.df$price)    #2 way table: row by column
c.mat   # row: predicted ;   column: actual
sum(diag(c.mat))/sum(c.mat) # this gives accuracy: overall correct classification percentage
c.mat[2,2]/sum(c.mat[,2])   # this gives True positive percentage, or sensitivity
c.mat[1,1]/sum(c.mat[,1])   # this gives True negatie percentage, or specificity
#### classification matrix: validation data
pred <- predict(nn, valid.df)
c.mat <- table(ifelse(pred[,1] > 0.5, 1, 0), valid.df$price)    #2 way table: row by column
c.mat   # row: predicted ;   column: actual
sum(diag(c.mat))/sum(c.mat) # this gives accuracy: overall correct classification percentage
c.mat[2,2]/sum(c.mat[,2])   # this gives True positive percentage, or sensitivity
c.mat[1,1]/sum(c.mat[,1])   # this gives True negatie percentage, or specificity
#  2a:  one layer 2 neurons
# nn <- neuralnet(Loan1 + Loan0 ~ Income + Education , data = train.df, linear.output = F, hidden = 10)
#  2b:  one layer 5 neurons
nn <- neuralnet(price1 + price0 ~ bedrooms + sqft_living + floors + yr_built , data = train.df, linear.output = F, hidden = 5)
clearPushBack()
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
View(house.df)
str(house.df)
levels(as.factor( house.df$city) )
house.df<-house.df[-4351,]
house.df<-house.df[-4347,]
house.df <- house.df[house.df$price != 0, ]
house.df <- na.omit(house.df)
str(house.df)
install.packages("dplyr")
library(dplyr)
library(psych)
describe(house.df)
house.df$price <- ifelse(house.df$price > mean(house.df$price), 1, 0)
str(house.df)
set.seed(2)
train.index <- sample( c(1:dim(house.df)[1]), dim(house.df)[1]*0.6 )
selected.var <- c(2,3,4,5,6, 7, 8, 9, 10, 11,12,13,14)
train.df <- house.df[train.index, selected.var]
valid.df <- house.df[-train.index, selected.var]
str(train.df)
str(valid.df)
logit.reg <- glm(price ~ ., data = train.df, family = "binomial")
summary(logit.reg)
library(caret)    # Classfication And REgression Training
varImp(logit.reg)
plot( varImp(logit.reg) )
pred <- predict(logit.reg, valid.df, type = "response")
c.mat <- table(ifelse(pred > 0.5, 1, 0), valid.df$price)    #2 way table: row by column
c.mat
sum(diag(c.mat))/sum(c.mat)
c.mat[2,2]/sum(c.mat[,2])
c.mat[1,1]/sum(c.mat[,1])
pred <- predict(logit.reg, train.df, type = "response")
c.mat <- table(ifelse(pred > 0.5, 1, 0), train.df$price)    #2 way table: row by column
c.mat
sum(diag(c.mat))/sum(c.mat)
c.mat[2,2]/sum(c.mat[,2])
c.mat[1,1]/sum(c.mat[,1])
install.packages("MASS")
library(MASS)
logit.reg <- glm(price ~ ., data = train.df, family = "binomial")
step<-stepAIC(logit.reg)   #stepwise
step$anova
logit.reg <- glm(price ~ . - sqft_basement- sqft_lot- waterfront, data = train.df, family = "binomial")
summary(logit.reg)
library(caret)
varImp(logit.reg)
plot( varImp(logit.reg) )
logit.reg <- glm(price ~ bedrooms + bathrooms +sqft_living + floors +
view + condition +sqft_above + yr_built+yr_renovated, data = train.df, family = "binomial")
summary(logit.reg)
pred <- predict(logit.reg, valid.df, type = "response")
c.mat <- table(ifelse(pred > 0.5, 1, 0), valid.df$price)    #2 way table: row by column
c.mat   # row: predicted ;   column: actual
sum(diag(c.mat))/sum(c.mat)
c.mat[2,2]/sum(c.mat[,2])
c.mat[1,1]/sum(c.mat[,1])
######## classification matrix: training data
pred <- predict(logit.reg, train.df, type = "response")
c.mat <- table(ifelse(pred > 0.5, 1, 0), train.df$price)    #2 way table: row by column
c.mat
sum(diag(c.mat))/sum(c.mat) # this gives accuracy: overall correct classification percentage
c.mat[2,2]/sum(c.mat[,2])   # this gives True positive percentage, or sensitivity
c.mat[1,1]/sum(c.mat[,1])   # this gives True negatie percentage, or specificity
library(e1071)   # misc functions in stat, prob
install.packages("Rcpp")
install.packages("Rcpp")
library(Rcpp)    # R and C++ integration
library(pastecs)
library(pROC)
#ROC-curve for validation data
pred <- predict(logit.reg, valid.df, type = "response")
roc_score=roc(valid.df[,8], pred)                #AUC score
plot(roc_score ,main ="ROC curve -- Logistic Regression ")
# names(roc_score)
roc_score$auc
#ROC-curve for training data
pred <- predict(logit.reg, train.df, type = "response")
roc_score=roc(train.df[,8], pred)
plot(roc_score ,main ="ROC curve -- Logistic Regression ")
roc_score$auc
chooseCRANmirror()
chooseCRANmirror()
df <- read.csv("gg.csv")
View(df)
str(df)
levels(as.factor( df$city) )
df <- na.omit(df)
str(df)  # structure of the data frame
install.packages("dplyr")
library(dplyr)
library(psych)
describe(df)
##### partition data into training and validation data
set.seed(1)
train.index <- sample( c( 1:length(df[,1]) ), 0.6*length(df[,1]) )
selected.var <- c(2,3,4,5,6, 7, 8, 9, 10, 11,12,13)
## create training data
train.df <- df[train.index, selected.var]
accuracy(gg.lm.pred, valid.df$price)
accuracy(gg.lm.pred.t, train.df$price)
#### sometime taking log of Y may improve the normality of residuals, e.g.,
gg.lm <- lm(log(price) ~ . -yr_built, data = train.df)
summary(gg.lm)
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
View(house.df)
str(house.df)
levels(as.factor( house.df$city) )
levels(as.factor( house.df$statezip) )
levels(as.factor( house.df$street) )
house.df <- na.omit(house.df)
house.df<-house.df[-4351,]
house.df<-house.df[-4347,]
str(house.df)  # structure of the data frame
View(house.df)
library(dplyr)
library(psych)
describe(house.df)
length(house.df[,1])
set.seed(1)
train.index <- sample( c( 1:length(house.df[,1]) ), 0.6*length(house.df[,1]) )
selected.var <- c(2,3,4,5,6, 7, 8, 9, 10, 11,12,13)
train.df <- house.df[train.index, selected.var]
valid.df <- house.df[-train.index, selected.var]
str(train.df)
View(train.df)
str(valid.df)
house.lm <- lm(price ~ . , data = train.df)
summary(house.lm)
hist(resid(house.lm))
qqnorm(resid(house.lm))   # QQ plot of residuals
qqline(resid(house.lm))  # to check if points fall nicely onto the line
install.packages("forecast")
library(forecast)
house.lm.pred <- predict(house.lm, valid.df)
accuracy(house.lm.pred, valid.df$price)
house.lm.pred.t <- predict(house.lm, train.df)
accuracy(house.lm.pred.t, train.df$price)
install.packages("leaps")
library(leaps)
search <- regsubsets(price ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
method = "exhaustive")
plot(search, scale="adjr2")
house.lm <- lm( price ~ . -yr_built, data = train.df)
summary(house.lm)
hist(resid(house.lm))
qqnorm(resid(house.lm))   # QQ plot of residuals
qqline(resid(house.lm))
house.lm.pred <- predict(house.lm, valid.df)
accuracy(house.lm.pred, valid.df$price)
house.lm.pred.t <- predict(house.lm, train.df)
accuracy(house.lm.pred.t, train.df$price)
log(10)
log(10000)
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
str(house.df)  # structure of the data frame
head(house.df, 9)
tail(house.df)   # default, show last 5 records
View(house.df)
summary(house.df)
library(psych)
describe(house.df)
## scatter plot with axes names
install.packages("plot")
plot(house.df$price ~ house.df$sqft_living, xlab = "Price", ylab = "Square Feet Living Area")
# alternative plot with ggplot
install.packages("ggplot2")
library(ggplot2)
ggplot(house.df) + geom_point(aes(x = price, y = sqft_living), colour = "navy", alpha = 0.7)
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
str(house.df)  # structure of the data frame
head(house.df, 9)
tail(house.df)   # default, show last 5 records
View(house.df)
summary(house.df)
library(psych)
describe(house.df)
## scatter plot with axes names
install.packages("plot")
plot(house.df$price ~ house.df$sqft_living, xlab = "Price", ylab = "Square Feet Living Area")
# alternative plot with ggplot
install.packages("ggplot2")
library(ggplot2)
ggplot(house.df) + geom_point(aes(x = price, y = sqft_living), colour = "navy", alpha = 0.7)
install.packages("plotly")
library(plotly)
# histogram with smoothed density
install.packages("plotly")
library(plotly)
p <- ggplot(house.df, aes(x=price)) +
geom_histogram(aes(y = ..density..), binwidth=density(house.df$price)$bw) +
geom_density(fill="red", alpha = 0.2)
ggplotly(p)
# add linear regression fit
p <- ggplot(house.df, aes(x = price, y = sqft_living) ) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm, se=TRUE)    # Add linear regression line
geom_smooth()            # Add a loess smoothed fit curve with confidence region
# loess (locally estimated scatterplot smoothing)
# lowess (locally weighted scatterplot smoothing)
ggplotly(p)
plot(house.df[,c(2,5,7,10)] )
install.packages("GGally")
library(GGally)
# ggpairs(housing.df[,c(2,3,4,5)] )
ggpairs(housing.df[,c(2:9)] )     # variables in columns 2 to 9
#ggpairs(housing.df[,c(2:4, 6, 8:10)] )
## barchart of City vs. mean Price
# compute mean Price Each City = (0, 1)
data.for.plot <- aggregate(house.df$price, by = list(house.df$city), FUN = mean)
data.for.plot
names(data.for.plot) <- c("city", "Meanprice")
barplot(data.for.plot$Meanprice,  names.arg = data.for.plot$city,
xlab = "city", ylab = "Avg. Price")
# alternative plot with ggplot
ggplot(data.for.plot) + geom_bar(aes(x = city, y = Meanprice), stat = "identity")
## barchart of city Vs price
data.for.plot <- aggregate(house.df$price, by = list(house.df$city), FUN = mean)
data.for.plot
names(data.for.plot) <- c("city", "Meanprice")
barplot(data.for.plot$Meanprice * 100,  names.arg = data.for.plot$city,
xlab = "city", ylab = "% of Meanprice")
#### Figure 3.2
## histogram of yr_built
hist(house.df$yr_built, xlab = "Year Built")
# alternative plot with ggplot
library(ggplot2)
ggplot(house.df) + geom_histogram(aes(x = yr_built), binwidth = 5)
ggplot(house.df) + geom_boxplot( aes( y = price) )
boxplot(house.df$price ~ house.df$waterfront, xlab = "waterfront", ylab = "price")
# alternative plot with ggplot
ggplot(house.df) + geom_boxplot(aes(x = as.factor(waterfront), y = price , color =as.factor(waterfront) )) + xlab("price")
p <- ggplot(housing.df, aes(x=yr_built)) +
geom_histogram(aes(y = ..density..), binwidth=density(housing.df$yr_built)$bw) +
geom_density(fill="red", alpha = 0.2)
ggplotly(p)
# animation
df <- data.frame(
x = c(1:10),
y = c(1:10),
f = c(1:10)
)
p <- ggplot(df, aes(x, y)) +
geom_point(aes(frame = f))
ggplotly(p)
# animation
# https://plot.ly/ggplot2/cumulative-animations/
# violin plot
# violin plot is similar to boxplot, with a kernel density plot on each side
ggplot(house.df) + geom_violin(aes(x = as.factor(waterfront), y = price)) + xlab("waterfront")
#### Figure 3.3
## side-by-side boxplots
# use par() to split the plots into panels.
par(mfcol = c(1, 4))
boxplot(housing.df$yr_built ~ housing.df$country, xlab = "country", ylab = "yr_built")
boxplot(housing.df$sqft_living ~ housing.df$country, xlab = "country", ylab = "sqft_living")
boxplot(housing.df$condition ~ housing.df$country, xlab = "country", ylab = "condition")
boxplot(housing.df$bedrooms ~ housing.df$country, xlab = "country", ylab = "bedrooms")
#### Figure 3.4
## simple heatmap of correlations (without values)
numeric_df <- na.omit(housing.df[sapply(housing.df, is.numeric)])
heatmap(cor(numeric_df), Rowv = NA, Colv = NA)
## heatmap with values
install.packages("reshape")
dev.off()
library(gplots)
heatmap.2(cor(numeric_df), #Rowv = FALSE, Colv = FALSE, dendrogram = "none",
cellnote = round(cor(numeric_df),2),
notecol = "black", key = FALSE, trace = 'none', margins = c(10,10))
install.packages("MASS")
library(MASS)
parcoord(numeric_df)
parcoord(numeric_df, col = rainbow(100), lty = 1, lwd = 2, var.label = TRUE)
install.packages("ggplot2")
install.packages("plotly")
install.packages("MASS")
install.packages("reshape")
install.packages("MASS")
install.packages("reshape")
install.packages("plotly")
chooseCRANmirror()
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
View(house.df)
str(house.df)
levels(as.factor( house.df$city) )
house.df<-house.df[-4351,]
house.df<-house.df[-4347,]
house.df <- na.omit(house.df)
str(house.df)  # structure of the data frame
house.df$price<- ifelse(house.df$price==0,0.0001,house.df$price)
install.packages("dplyr")
install.packages("psych")
library(dplyr)
library(psych)
describe(house.df)
##### partition data into training and validation data
set.seed(1)
train.index <- sample( c( 1:length(house.df[,1]) ), 0.6*length(house.df[,1]) )
selected.var <- c(2,3,4,5,6, 7, 8, 9, 10, 11,12,13)
## create training data
train.df <- house.df[train.index, selected.var]
## create validation data
valid.df <- house.df[-train.index, selected.var]    # "-" works for numbers; otherwise setdiff
str(train.df)
str(valid.df)
house.lm <- lm(price ~ . , data = train.df)    # lm can account for categorical variables
# display regression results
summary(house.lm)
## check the normality of residuals
hist(resid(house.lm))
# QQ plot of residuals: another check of residuals' normality
qqnorm(resid(house.lm))   # QQ plot of residuals
qqline(resid(house.lm))  # to check if points fall nicely onto the line
#### get accuracy measures of the regression model in the training and validationd data
install.packages("forecast")
library(forecast)
# use predict() to make predictions Y^ on the validation set.
house.lm.pred <- predict(house.lm, valid.df)
# display accuracy measures  for the validation data
accuracy(house.lm.pred, valid.df$price)
# compare to the accuracy measures  for the training data
house.lm.pred.t <- predict(house.lm, train.df)
accuracy(house.lm.pred.t, train.df$price)
#### Perform variable selection
# use regsubsets() in package leaps to run an exhaustive search.
# like lm, categorical predictors are accepted
# if needed, can be turned into dummies manually using  e.g. factor(  )
install.packages("leaps")
library(leaps)
# variable selection using "regsubsets"
search <- regsubsets(price ~ ., data = train.df, nbest = 1, nvmax = dim(train.df)[2],
method = "exhaustive")
#graphical display of selection results, based on adjusted R-sq
plot(search, scale="adjr2")
#graphical display of selection results, based on BIC
# BIC is an extension of AIC=-2lnL + 2*k; prefer small AIC
# plot(search)  # default is using BIC
str(train.df)
### create the indicator variables in the train.df and valid.df
# if there is categorical variable, to shows how many levels in Fuel_Type
str(valid.df)
###
# Now estimate the model chosen by regsubsets:
# can run the model with the variables you want to include using "+", and don't want to include using "-"
#    now, since Fuel_Type and the 3 indicators are both in the data frame "train.df",
#    remove the Fuel_Type which is not needed now and add the levels you want to include, e.g.
house.lm <- lm( price ~ bedrooms + bathrooms +sqft_living + sqft_lot + floors +
waterfront + view + condition +sqft_above + sqft_basement, data = train.df)
summary(house.lm)
# alternatively, you can run the model with everything in train.df using "." and use "-" to exclude
# the variables you don't want to include, e.g.,
#  since Fuel_Type and the 3 indicators are both in the data frame "train.df",
#  remove the original categorical variable Fuel_Type and the levels you don't want to included, e.g.
house.lm <- lm( price ~ . -yr_built, data = train.df)
summary(house.lm)
# if you don't have categorical variable and no indicators are created, simply use "-" to
# exclude the variables you don't want to include, e.g.,
# house.lm <- lm( Price ~ . -Met_Color  , data = train.df)
# summary(house.lm)
## check the normality of residuals
hist(resid(house.lm))
# QQ plot of residuals: another check of residuals' normality
qqnorm(resid(house.lm))   # QQ plot of residuals
qqline(resid(house.lm))  # to check if points fall nicely onto the line
#### get accuracy measures of the regression model in the training and validationd data
# install.packages("forecast")
# library(forecast)
# use predict() to make predictions Y^ on the validation set.
house.lm.pred <- predict(house.lm, valid.df)
accuracy(house.lm.pred, valid.df$price)
# compare to the accuracy measures  for the training data
house.lm.pred.t <- predict(house.lm, train.df)
accuracy(house.lm.pred.t, train.df$price)
#### sometime taking log of Y may improve the normality of residuals, e.g.,
house.lm <- lm(log(price) ~ . -yr_built, data = train.df)
summary(house.lm)
install.packages("dplyr")
install.packages("psych")
install.packages("psych")
install.packages("dplyr")
git
house.df <- read.csv("RatanTejaPunati_housing_price.csv")
str(house.df)  # structure of the data frame
head(house.df, 9)
tail(house.df)   # default, show last 5 records
summary(house.df)
library(psych)
describe(house.df)
plot(house.df$price ~ house.df$sqft_living, xlab = "Price", ylab = "Square Feet Living Area")
library(ggplot2)
ggplot(house.df) + geom_point(aes(x = price, y = sqft_living), colour = "navy", alpha = 0.7)
library(plotly)
library(plotly)
p <- ggplot(house.df, aes(x=price)) +
geom_histogram(aes(y = ..density..), binwidth=density(house.df$price)$bw) +
geom_density(fill="red", alpha = 0.2)
ggplotly(p)
# add linear regression fit
p <- ggplot(house.df, aes(x = price, y = sqft_living) ) +
geom_point(shape=1) +    # Use hollow circles
geom_smooth(method=lm, se=TRUE)    # Add linear regression line
geom_smooth()            # Add a loess smoothed fit curve with confidence region
ggplotly(p)
plot(house.df[,c(2,5,7,10)] )
library(GGally)
# ggpairs(housing.df[,c(2,3,4,5)] )
ggpairs(housing.df[,c(2:9)] )     # variables in columns 2 to 9
## barchart of City vs. mean Price
# compute mean Price Each City = (0, 1)
data.for.plot <- aggregate(house.df$price, by = list(house.df$city), FUN = mean)
data.for.plot
names(data.for.plot) <- c("city", "Meanprice")
barplot(data.for.plot$Meanprice,  names.arg = data.for.plot$city,
xlab = "city", ylab = "Avg. Price")
# alternative plot with ggplot
ggplot(data.for.plot) + geom_bar(aes(x = city, y = Meanprice), stat = "identity")
## barchart of city Vs price
data.for.plot <- aggregate(house.df$price, by = list(house.df$city), FUN = mean)
data.for.plot
names(data.for.plot) <- c("city", "Meanprice")
barplot(data.for.plot$Meanprice * 100,  names.arg = data.for.plot$city,
xlab = "city", ylab = "% of Meanprice")
## histogram of yr_built
hist(house.df$yr_built, xlab = "Year Built")
# alternative plot with ggplot
library(ggplot2)
ggplot(house.df) + geom_histogram(aes(x = yr_built), binwidth = 5)
ggplot(house.df) + geom_boxplot( aes( y = price) )
boxplot(house.df$price ~ house.df$waterfront, xlab = "waterfront", ylab = "price")
ggplot(house.df) + geom_boxplot(aes(x = as.factor(waterfront), y = price , color =as.factor(waterfront) )) + xlab("price")
p <- ggplot(housing.df, aes(x=yr_built)) +
geom_histogram(aes(y = ..density..), binwidth=density(housing.df$yr_built)$bw) +
geom_density(fill="red", alpha = 0.2)
ggplotly(p)
# animation
df <- data.frame(
x = c(1:10),
y = c(1:10),
f = c(1:10)
)
p <- ggplot(df, aes(x, y)) +
geom_point(aes(frame = f))
ggplotly(p)
# violin plot
# violin plot is similar to boxplot, with a kernel density plot on each side
ggplot(house.df) + geom_violin(aes(x = as.factor(waterfront), y = price)) + xlab("waterfront")
## side-by-side boxplots
# use par() to split the plots into panels.
par(mfcol = c(1, 4))
boxplot(housing.df$yr_built ~ housing.df$country, xlab = "country", ylab = "yr_built")
boxplot(housing.df$sqft_living ~ housing.df$country, xlab = "country", ylab = "sqft_living")
boxplot(housing.df$condition ~ housing.df$country, xlab = "country", ylab = "condition")
boxplot(housing.df$bedrooms ~ housing.df$country, xlab = "country", ylab = "bedrooms")
#### Figure 3.4
## simple heatmap of correlations (without values)
numeric_df <- na.omit(housing.df[sapply(housing.df, is.numeric)])
heatmap(cor(numeric_df), Rowv = NA, Colv = NA)
## heatmap with values
install.packages("reshape")
install.packages("reshape")
dev.off()
library(gplots)
heatmap.2(cor(numeric_df), #Rowv = FALSE, Colv = FALSE, dendrogram = "none",
cellnote = round(cor(numeric_df),2),
notecol = "black", key = FALSE, trace = 'none', margins = c(10,10))
library(MASS)
parcoord(numeric_df)
parcoord(numeric_df, col = rainbow(100), lty = 1, lwd = 2, var.label = TRUE)
